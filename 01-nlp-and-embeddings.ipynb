{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199cdfb6",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "- undertand how text is processed and analysed\n",
    "\n",
    "# NLP (Natural Language Processing)\n",
    "\n",
    "NLP is analysing and generating text(Language).\n",
    "\n",
    "- **analysis:** extract meaning, classify, and transalate \n",
    "\n",
    "- **generation:** create text, summarize, and chat\n",
    "\n",
    "\n",
    "\n",
    "### Text Processing\n",
    "\n",
    "**tokens:** the basic text units a language model prcoesses, sometimes words, sometimes parts of words, they may not always be meaningful individually, but the meaning is formed across sequence of tokens.\n",
    "\n",
    "\n",
    "**reference:**\n",
    "\n",
    "[nltk](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c524cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.14/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.14/site-packages (1.8.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.14/site-packages (2.3.3)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.14/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.14/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.14/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.14/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.24.1 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.14/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d296342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Don't split this! Dr. Smith's email is test@example.com. Cost: $49.99\n",
      "\n",
      "Naive Tokenization (split by whitespace):\n",
      "[\"Don't\", 'split', 'this!', 'Dr.', \"Smith's\", 'email', 'is', 'test@example.com.', 'Cost:', '$49.99']\n",
      "\n",
      "NLTK word_tokenize:\n",
      "['Do', \"n't\", 'split', 'this', '!', 'Dr.', 'Smith', \"'s\", 'email', 'is', 'test', '@', 'example.com', '.', 'Cost', ':', '$', '49.99']\n",
      "============================================================\n",
      "EDGE CASES\n",
      "\n",
      "Original: It's can't won't\n",
      "Tokens: ['It', \"'s\", 'ca', \"n't\", 'wo', \"n't\"]\n",
      "\n",
      "Original: U.S.A. vs USA\n",
      "Tokens: ['U.S.A.', 'vs', 'USA']\n",
      "\n",
      "Original: covid-19\n",
      "Tokens: ['covid-19']\n",
      "\n",
      "Original: test@email.com\n",
      "Tokens: ['test', '@', 'email.com']\n",
      "\n",
      "Original: I'm feeling üòä today!\n",
      "Tokens: ['I', \"'m\", 'feeling', 'üòä', 'today', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishnagopikaurlaganti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TEXT PREPROCESSING: Tokenization\n",
    "\n",
    "\n",
    "def demo_tokenization():\n",
    "    \n",
    "    text = \"Don't split this! Dr. Smith's email is test@example.com. Cost: $49.99\"\n",
    "\n",
    "\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "\n",
    "    # Naive approach: split by whitespace\n",
    "    print(\"\\nNaive Tokenization (split by whitespace):\")\n",
    "    print(text.split())\n",
    "\n",
    "    # NLTK word_tokenize\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    print(\"\\nNLTK word_tokenize:\")\n",
    "    print(word_tokenize(text))\n",
    "\n",
    "\n",
    "    \n",
    "    # Edge cases\n",
    "    print(\"=\"*60)\n",
    "    print(\"EDGE CASES\")\n",
    "\n",
    "\n",
    "    edge_cases = [\n",
    "        \"It's can't won't\",\n",
    "        \"U.S.A. vs USA\",\n",
    "        \"covid-19\",\n",
    "        \"test@email.com\",\n",
    "        \"I'm feeling üòä today!\",\n",
    "    ]\n",
    "\n",
    "    for case in edge_cases:\n",
    "        print(f\"\\nOriginal: {case}\")\n",
    "        print(\"Tokens:\", word_tokenize(case))\n",
    "    \n",
    "\n",
    "demo_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e610be5",
   "metadata": {},
   "source": [
    "A better tokenizer handeles contractions, seperated punctuation and keeps meaningful units together\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f1053",
   "metadata": {},
   "source": [
    "### Notmalization\n",
    "\n",
    "sometimes we have tokens that have same surface forms but convey the same underlying meaning, so we normalize them\n",
    "\n",
    "**example:** run, runs, ran, running -> run\n",
    "\n",
    "\n",
    "this is done via:\n",
    "\n",
    "- **Stemming**\n",
    "- **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39bc8b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming vs Lemmatization\n",
      "Word           Stemmed        Lemmatized     \n",
      "running        run            run            \n",
      "runs           run            run            \n",
      "ran            ran            run            \n",
      "better         better         better         \n",
      "best           best           best           \n",
      "good           good           good           \n",
      "caring         care           care           \n",
      "cares          care           care           \n",
      "cared          care           care           \n",
      "studies        studi          study          \n",
      "studying       studi          study          \n",
      "studied        studi          study          \n"
     ]
    }
   ],
   "source": [
    "#TEXT PREPROCESSING: Normalization\n",
    "\n",
    "\n",
    "\n",
    "def demo_normalization():\n",
    "    \"\"\"Compare stemming and lemmatization\"\"\"\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    print(\"Stemming vs Lemmatization\")\n",
    "\n",
    "    words = [\n",
    "        'running', 'runs', 'ran',\n",
    "        'better', 'best', 'good',\n",
    "        'caring', 'cares', 'cared',\n",
    "        'studies', 'studying', 'studied'\n",
    "    ]\n",
    "\n",
    "    print(f\"{'Word':<15}{'Stemmed':<15}{'Lemmatized':<15}\")\n",
    "\n",
    "    for word in words:\n",
    "        stemmed = stemmer.stem(word)\n",
    "        lemma_v = lemmatizer.lemmatize(word, pos='v')  # specify verb for better results\n",
    "        lemma_n = lemmatizer.lemmatize(word, pos='n')  # specify noun for better results\n",
    "\n",
    "        lemma = lemma_v if lemma_v != word else lemma_n\n",
    "\n",
    "        print(f\"{word:<15}{stemmed:<15}{lemma:<15}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "demo_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44e7bc",
   "metadata": {},
   "source": [
    "**STEMMING (Porter Stemmer):**\n",
    "\n",
    "- Fast - just chops off endings\n",
    "- Good for search (retrieval)\n",
    "- Creates non-words: 'caring' ‚Üí 'care' (good), 'studies' ‚Üí 'studi' (bad)\n",
    "    \n",
    "**LEMMATIZATION:**\n",
    "- Real words - uses dictionary\n",
    "- Better for analysis\n",
    "- Slower\n",
    "- Needs part-of-speech tag\n",
    "    \n",
    "\n",
    "**WHEN TO USE:**\n",
    "- Search engines ‚Üí Stemming (speed)\n",
    "- Sentiment analysis ‚Üí Lemmatization (accuracy)\n",
    "\n",
    "----\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870687a",
   "metadata": {},
   "source": [
    "### N-grams & Language Modeling\n",
    "\n",
    "until now we processed text now lets predict what comes next in a sentence\n",
    "\n",
    "usecases:\n",
    "\n",
    "- auto complete\n",
    "\n",
    "An n-gram model is based on conditional probability: given the previous words, it estimates what word is likely to occur next.\n",
    "\n",
    "- n=1 (unigram)\n",
    "- n=2 (bigram)\n",
    "- n = 3 (trigram) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eaf36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "N-GRAM LANGUAGE MODELS\n",
      "============================================================\n",
      "\n",
      "Training corpus:\n",
      "\n",
      "    To be or not to be, that is the question.\n",
      "    All the world's a stage, and all the men and women merely players.\n",
      "    To be or not to be, to thine own self be true.\n",
      "    The course of true love nev...\n",
      "\n",
      "\n",
      "============================================================\n",
      "2-GRAM MODEL (context = 1 words)\n",
      "============================================================\n",
      "\n",
      "Probabilities after 'to':\n",
      "  P(be | to) = 0.800\n",
      "  P(thine | to) = 0.200\n",
      "\n",
      "Generated text:\n",
      "  1. to thine own self be true . all that is not to be , to be , and all the\n",
      "  2. to be true . all the question . the question . what 's a rose by any other name ?\n",
      "  3. to be , that is not to be true love never did run smooth .\n",
      "\n",
      "============================================================\n",
      "3-GRAM MODEL (context = 2 words)\n",
      "============================================================\n",
      "\n",
      "Generated text:\n",
      "  1. to be or not to be , that is the question . all that glitters is not gold . what\n",
      "  2. to be or not to be or not to be , that is the question . all the world 's\n",
      "  3. to be , to thine own self be true . the course of true love never did run smooth .\n"
     ]
    }
   ],
   "source": [
    "# N-GRAMS: Modeling sequences of words\n",
    "# Predicting the next word based on previous words\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    Simple n-gram language model\n",
    "    Predicts next word based on previous n-1 words\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=2):\n",
    "        \"\"\"\n",
    "        n=1: unigram (no context)\n",
    "        n=2: bigram (previous 1 word)\n",
    "        n=3: trigram (previous 2 words)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.ngrams = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "    \n",
    "    def train(self, text):\n",
    "        \"\"\"Learn n-gram probabilities from text\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Add start/end markers\n",
    "        tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
    "        \n",
    "        # Count n-grams\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            # Context: first n-1 words\n",
    "            context = tuple(tokens[i:i + self.n - 1])\n",
    "            # Next word\n",
    "            next_word = tokens[i + self.n - 1]\n",
    "            \n",
    "            self.ngrams[context][next_word] += 1\n",
    "            self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, context, word):\n",
    "        \"\"\"P(word | context)\"\"\"\n",
    "        context = tuple(context)\n",
    "        if context not in self.ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        count = self.ngrams[context][word]\n",
    "        total = self.context_counts[context]\n",
    "        return count / total\n",
    "    \n",
    "    def generate(self, max_words=20):\n",
    "        \"\"\"Generate text using the model\"\"\"\n",
    "        context = ['<START>'] * (self.n - 1)\n",
    "        result = []\n",
    "        \n",
    "        for _ in range(max_words):\n",
    "            # Get possible next words\n",
    "            context_tuple = tuple(context[-(self.n-1):])\n",
    "            \n",
    "            if context_tuple not in self.ngrams:\n",
    "                break\n",
    "            \n",
    "            # Choose next word based on probabilities\n",
    "            next_words = self.ngrams[context_tuple]\n",
    "            next_word = random.choices(\n",
    "                list(next_words.keys()),\n",
    "                weights=list(next_words.values())\n",
    "            )[0]\n",
    "            \n",
    "            if next_word == '<END>':\n",
    "                break\n",
    "            \n",
    "            result.append(next_word)\n",
    "            context.append(next_word)\n",
    "        \n",
    "        return ' '.join(result)\n",
    "\n",
    "\n",
    "def demo_ngrams():\n",
    "    \"\"\"Show n-grams in action\"\"\"\n",
    "    \n",
    "    # Training data: Shakespeare quotes\n",
    "    corpus = \"\"\"\n",
    "    To be or not to be, that is the question.\n",
    "    All the world's a stage, and all the men and women merely players.\n",
    "    To be or not to be, to thine own self be true.\n",
    "    The course of true love never did run smooth.\n",
    "    All that glitters is not gold.\n",
    "    What's in a name? A rose by any other name would smell as sweet.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"N-GRAM LANGUAGE MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTraining corpus:\\n{corpus[:200]}...\\n\")\n",
    "    \n",
    "    # Train different n-gram models\n",
    "    for n in [2, 3]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{n}-GRAM MODEL (context = {n-1} words)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        model = NGramModel(n=n)\n",
    "        model.train(corpus)\n",
    "        \n",
    "        # Show some probabilities\n",
    "        if n == 2:\n",
    "            context = ['to']\n",
    "            print(f\"\\nProbabilities after '{context[0]}':\")\n",
    "            for word, count in model.ngrams[tuple(context)].most_common(5):\n",
    "                prob = model.probability(context, word)\n",
    "                print(f\"  P({word} | {context[0]}) = {prob:.3f}\")\n",
    "        \n",
    "        # Generate text\n",
    "        print(f\"\\nGenerated text:\")\n",
    "        for i in range(3):\n",
    "            print(f\"  {i+1}. {model.generate()}\")\n",
    "demo_ngrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ee056",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Bigrams capture local patterns\n",
    "- Trigrams more coherent but need more data\n",
    "- Still nonsensical - no real understanding\n",
    "- Can only use patterns seen in training (data sparicty problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb1a8b",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "experiment with your own n-gram\n",
    "\n",
    "[n-gram](./n-gram-experiment.py)\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1ae55",
   "metadata": {},
   "source": [
    "N-grams predict the next word. But what if we want to classify an entire document, like 'is this review positive or negative?' We need a different approach.\n",
    "\n",
    "\n",
    "#### Bag of Words & Classification\n",
    "\n",
    "from sequences to documents\n",
    "\n",
    "one-hot encode the each token and sum them across the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b06cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: len(vocab) = 8\n",
      "['great', 'hate', 'is', 'it', 'love', 'movie', 'terrible', 'this']\n",
      "Document Vectors (BoW)\n",
      "   great  hate  is  it  love  movie  terrible  this\n",
      "0      0     0   0   0     1      1         0     1\n",
      "1      0     1   0   0     0      1         0     1\n",
      "2      1     0   1   1     1      1         0     1\n",
      "3      0     1   1   1     0      1         1     1\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS: Representing documents as word counts\n",
    "# Ignore order, just count presence\n",
    "\n",
    "def demo_bow():\n",
    "    \"\"\"Visualize bag of words representation\"\"\"\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    documents = [\n",
    "        \"I love this movie.\",\n",
    "        \"I hate this movie.\",\n",
    "        \"this is a great movie, I love it!\",\n",
    "        \"this is a terrible movie, I hate it!\"\n",
    "    ]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    print(f\"Vocabulary: len(vocab) = {len(vocab)}\")\n",
    "    print(sorted(vocab))\n",
    "\n",
    "    print(\"Document Vectors (BoW)\")\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(X.toarray(), columns=vocab)\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "    \n",
    "demo_bow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fb1b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTIMENT CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 39\n",
      "Number of reviews: 16\n",
      "Feature matrix shape: (16, 39)\n",
      "\n",
      "Accuracy: 20.00%\n",
      "\n",
      "============================================================\n",
      "MOST IMPORTANT WORDS\n",
      "============================================================\n",
      "\n",
      "Positive indicators:\n",
      "  wonderful       ‚Üí +0.639\n",
      "  great           ‚Üí +0.493\n",
      "  best            ‚Üí +0.413\n",
      "  recommend       ‚Üí +0.311\n",
      "  fantastic       ‚Üí +0.311\n",
      "\n",
      "Negative indicators:\n",
      "  terrible        ‚Üí -0.771\n",
      "  awful           ‚Üí -0.307\n",
      "  bad             ‚Üí -0.307\n",
      "  worst           ‚Üí -0.276\n",
      "  watch           ‚Üí -0.276\n",
      "\n",
      "============================================================\n",
      "TESTING ON NEW EXAMPLES\n",
      "============================================================\n",
      "\n",
      "'This film was great'\n",
      "  ‚Üí NEGATIVE (51.6% confident)\n",
      "\n",
      "'Absolutely terrible'\n",
      "  ‚Üí NEGATIVE (64.0% confident)\n",
      "\n",
      "'I enjoyed it'\n",
      "  ‚Üí POSITIVE (54.9% confident)\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è  THE SYNONYM PROBLEM\n",
      "============================================================\n",
      "\n",
      "Watch what happens with synonyms:\n",
      "\n",
      "'This movie was great' (seen 'great')\n",
      "  ‚Üí NEGATIVE (52.0%)\n",
      "\n",
      "'This film was excellent' (never saw 'excellent'!)\n",
      "  ‚Üí NEGATIVE (63.5%)\n",
      "\n",
      "'I loved this movie' (seen 'loved')\n",
      "  ‚Üí NEGATIVE (55.0%)\n",
      "\n",
      "'I adored this film' (never saw 'adored'!)\n",
      "  ‚Üí NEGATIVE (59.7%)\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT\n",
      "============================================================\n",
      "\n",
      "    The model treats 'great', 'excellent', 'wonderful' as\n",
      "    completely separate dimensions!\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SENTIMENT CLASSIFICATION using Bag of Words\n",
    "Classify movie reviews as positive/negative\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def demo_classifier():\n",
    "    \"\"\"Build and evaluate a sentiment classifier\"\"\"\n",
    "    \n",
    "    # Sample data (in reality, you'd use thousands of reviews)\n",
    "    reviews = [\n",
    "        # Positive\n",
    "        \"This movie was excellent and amazing\",\n",
    "        \"I loved every minute of this film\",\n",
    "        \"Great acting and wonderful story\",\n",
    "        \"Fantastic movie, highly recommend\",\n",
    "        \"Best film I've seen this year\",\n",
    "        \"Brilliant and entertaining\",\n",
    "        \"Absolutely loved it, great cast\",\n",
    "        \"Wonderful cinematography and plot\",\n",
    "        # Negative\n",
    "        \"This movie was terrible and boring\",\n",
    "        \"I hated every minute of this film\",\n",
    "        \"Bad acting and awful story\",\n",
    "        \"Worst movie, don't watch\",\n",
    "        \"Terrible film I've seen this year\",\n",
    "        \"Horrible and disappointing\",\n",
    "        \"Absolutely hated it, bad cast\",\n",
    "        \"Terrible cinematography and plot\",\n",
    "    ]\n",
    "    \n",
    "    labels = [1, 1, 1, 1, 1, 1, 1, 1,  # positive\n",
    "              0, 0, 0, 0, 0, 0, 0, 0]  # negative\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"SENTIMENT CLASSIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Convert to BoW\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    print(f\"\\nVocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"Number of reviews: {len(reviews)}\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    # Train classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Show most important features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefficients = classifier.coef_[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MOST IMPORTANT WORDS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Top positive words\n",
    "    top_positive_idx = coefficients.argsort()[-5:][::-1]\n",
    "    print(\"\\nPositive indicators:\")\n",
    "    for idx in top_positive_idx:\n",
    "        print(f\"  {feature_names[idx]:15} ‚Üí {coefficients[idx]:+.3f}\")\n",
    "    \n",
    "    # Top negative words\n",
    "    top_negative_idx = coefficients.argsort()[:5]\n",
    "    print(\"\\nNegative indicators:\")\n",
    "    for idx in top_negative_idx:\n",
    "        print(f\"  {feature_names[idx]:15} ‚Üí {coefficients[idx]:+.3f}\")\n",
    "    \n",
    "    # Test on new examples\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING ON NEW EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_examples = [\n",
    "        \"This film was great\",\n",
    "        \"Absolutely terrible\",\n",
    "        \"I enjoyed it\",\n",
    "    ]\n",
    "    \n",
    "    X_new = vectorizer.transform(test_examples)\n",
    "    predictions = classifier.predict(X_new)\n",
    "    probabilities = classifier.predict_proba(X_new)\n",
    "    \n",
    "    for text, pred, prob in zip(test_examples, predictions, probabilities):\n",
    "        sentiment = \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
    "        confidence = prob[pred]\n",
    "        print(f\"\\n'{text}'\")\n",
    "        print(f\"  ‚Üí {sentiment} ({confidence:.1%} confident)\")\n",
    "    \n",
    "    # NOW THE PROBLEM!\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö†Ô∏è  THE SYNONYM PROBLEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    problem_examples = [\n",
    "        (\"This movie was great\", \"seen 'great'\"),\n",
    "        (\"This film was excellent\", \"never saw 'excellent'!\"),\n",
    "        (\"I loved this movie\", \"seen 'loved'\"),\n",
    "        (\"I adored this film\", \"never saw 'adored'!\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nWatch what happens with synonyms:\")\n",
    "    X_problem = vectorizer.transform([ex[0] for ex in problem_examples])\n",
    "    predictions = classifier.predict(X_problem)\n",
    "    probabilities = classifier.predict_proba(X_problem)\n",
    "    \n",
    "    for (text, note), pred, prob in zip(problem_examples, predictions, probabilities):\n",
    "        sentiment = \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
    "        confidence = prob[pred]\n",
    "        print(f\"\\n'{text}' ({note})\")\n",
    "        print(f\"  ‚Üí {sentiment} ({confidence:.1%})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY INSIGHT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    The model treats 'great', 'excellent', 'wonderful' as\n",
    "    completely separate dimensions!\n",
    "    \"\"\")\n",
    "\n",
    "demo_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae601e",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Each document is now a vector of word counts\n",
    "- Same length (vocabulary size)\n",
    "- Can now use machine learning\n",
    "    \n",
    "- Lost word order: \"dog bites man\" = \"man bites dog\"\n",
    "- Lost syntax: \"not good\" looks like \"good\"\n",
    "- Every word is a separate dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccfdac",
   "metadata": {},
   "source": [
    "**1. TEXT PREPROCESSING**\n",
    "\n",
    "- Tokenization: Breaking text into units\n",
    "- Normalization: Stemming/lemmatization\n",
    "- These are fundamental to all NLP\n",
    "    \n",
    "**2. N-GRAMS**\n",
    "\n",
    "- Model probability of word sequences\n",
    "- Predict next word\n",
    "- Can't generalize to unseen sequences\n",
    "- Data sparsity problem\n",
    "    \n",
    "**3. BAG OF WORDS + CLASSIFICATION**\n",
    "- Represent documents as vectors\n",
    "- Train classifiers (logistic regression)\n",
    "- Each word is separate dimension\n",
    "- Synonyms are invisible\n",
    "- Vocabulary explosion (50,000+ dimensions)\n",
    "\n",
    "\n",
    "\n",
    "current representation does not consider car and automobile as same thing \n",
    "\n",
    "'car'        ‚Üí [0, 0, 1, 0, 0, ...]  (50,000 dimensions)\n",
    "'automobile' ‚Üí [0, 1, 0, 0, 0, ...]  (completely different!)\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237fa5b",
   "metadata": {},
   "source": [
    "#### Distributional Hypothesis\n",
    "\n",
    "words that appear in similar contexts have similar meanings.\n",
    "\n",
    "*You shall know a word by the company it keeps - Firth, 1957*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5a6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THE DISTRIBUTIONAL HYPOTHESIS\n",
      "============================================================\n",
      "\n",
      "Scenario: You've never heard of 'ongchoi'\n",
      "But you see these sentences...\n",
      "\n",
      "  ‚Ä¢ Ongchoi is delicious sauteed with garlic.\n",
      "  ‚Ä¢ Ongchoi is superb over rice.\n",
      "  ‚Ä¢ Ongchoi leaves with salty sauces are great.\n",
      "\n",
      "What words appear near 'ongchoi'?\n",
      "\n",
      "Words near 'ongchoi':\n",
      "  is              ‚Üí 2 times\n",
      "  delicious       ‚Üí 1 times\n",
      "  sauteed         ‚Üí 1 times\n",
      "  superb          ‚Üí 1 times\n",
      "  over            ‚Üí 1 times\n",
      "  leaves          ‚Üí 1 times\n",
      "  with            ‚Üí 1 times\n",
      "  salty           ‚Üí 1 times\n",
      "\n",
      "============================================================\n",
      "COMPARE TO KNOWN WORDS\n",
      "============================================================\n",
      "\n",
      "Words near 'spinach':\n",
      "  sauteed         ‚Üí 1 times\n",
      "  with            ‚Üí 1 times\n",
      "  garlic          ‚Üí 1 times\n",
      "\n",
      "Words near 'chard':\n",
      "  stems           ‚Üí 1 times\n",
      "  and             ‚Üí 1 times\n",
      "  leaves          ‚Üí 1 times\n",
      "\n",
      "============================================================\n",
      "CONCLUSION\n",
      "============================================================\n",
      "\n",
      "    ongchoi, spinach, and chard all appear with:\n",
      "      ‚Ä¢ delicious, sauteed, garlic, rice, leaves\n",
      "\n",
      "    Even without knowing what 'ongchoi' is, we can infer:\n",
      "      ‚Üí It's probably a leafy green vegetable!\n",
      "\n",
      "    This is the DISTRIBUTIONAL HYPOTHESIS:\n",
      "      Words with similar contexts have similar meanings.\n",
      "\n",
      "    (It's actually Ipomoea aquatica, also called water spinach!)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#¬†THE DISTRIBUTIONAL HYPOTHESIS\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def demo_distributional_hypothesis():\n",
    "    \"\"\"Show how context reveals meaning\"\"\"\n",
    "    \n",
    "    # The famous example from your document\n",
    "    corpus = \"\"\"\n",
    "    Ongchoi is delicious sauteed with garlic.\n",
    "    Ongchoi is superb over rice.\n",
    "    Ongchoi leaves with salty sauces are great.\n",
    "    Spinach sauteed with garlic over rice is delicious.\n",
    "    Chard stems and leaves are delicious.\n",
    "    Collard greens and other salty leafy greens are healthy.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"THE DISTRIBUTIONAL HYPOTHESIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nScenario: You've never heard of 'ongchoi'\")\n",
    "    print(\"But you see these sentences...\\n\")\n",
    "    \n",
    "    # Show ongchoi contexts\n",
    "    for line in corpus.strip().split('\\n')[:3]:\n",
    "        if 'ongchoi' in line.lower():\n",
    "            print(f\"  ‚Ä¢ {line.strip()}\")\n",
    "    \n",
    "    print(\"\\nWhat words appear near 'ongchoi'?\")\n",
    "    \n",
    "    # Count context words\n",
    "    def get_context_words(corpus, target_word, window=3):\n",
    "        \"\"\"Get words that appear near target word\"\"\"\n",
    "        context_words = Counter()\n",
    "        \n",
    "        for sentence in corpus.lower().split('.'):\n",
    "            tokens = word_tokenize(sentence)\n",
    "            \n",
    "            for i, token in enumerate(tokens):\n",
    "                if token == target_word:\n",
    "                    # Get window around target\n",
    "                    start = max(0, i - window)\n",
    "                    end = min(len(tokens), i + window + 1)\n",
    "                    \n",
    "                    for j in range(start, end):\n",
    "                        if j != i:  # Skip the target itself\n",
    "                            context_words[tokens[j]] += 1\n",
    "        \n",
    "        return context_words\n",
    "    \n",
    "    ongchoi_context = get_context_words(corpus, 'ongchoi')\n",
    "    \n",
    "    print(\"\\nWords near 'ongchoi':\")\n",
    "    for word, count in ongchoi_context.most_common(10):\n",
    "        print(f\"  {word:15} ‚Üí {count} times\")\n",
    "    \n",
    "    # Compare to similar words\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARE TO KNOWN WORDS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    spinach_context = get_context_words(corpus, 'spinach')\n",
    "    chard_context = get_context_words(corpus, 'chard')\n",
    "    \n",
    "    print(\"\\nWords near 'spinach':\")\n",
    "    for word, count in spinach_context.most_common(5):\n",
    "        print(f\"  {word:15} ‚Üí {count} times\")\n",
    "    \n",
    "    print(\"\\nWords near 'chard':\")\n",
    "    for word, count in chard_context.most_common(5):\n",
    "        print(f\"  {word:15} ‚Üí {count} times\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONCLUSION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    ongchoi, spinach, and chard all appear with:\n",
    "      ‚Ä¢ delicious, sauteed, garlic, rice, leaves\n",
    "    \n",
    "    Even without knowing what 'ongchoi' is, we can infer:\n",
    "      ‚Üí It's probably a leafy green vegetable!\n",
    "    \n",
    "    This is the DISTRIBUTIONAL HYPOTHESIS:\n",
    "      Words with similar contexts have similar meanings.\n",
    "    \n",
    "    (It's actually Ipomoea aquatica, also called water spinach!)\n",
    "    \"\"\")\n",
    "\n",
    "demo_distributional_hypothesis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4a2ee",
   "metadata": {},
   "source": [
    "#### Co-occurance vectors\n",
    "\n",
    "A co-occurrence vector represents a word based on how frequently it appears together with other words in a fixed context window within a corpus. Each dimension corresponds to a word in the vocabulary, and the value in that dimension is the count (or weighted count) of how often the target word appears near that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca068133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WORD VECTORS: Representing words by their context\n",
    "# Building co-occurrence matrices\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def demo_cooccurrence_matrix():\n",
    "    \"\"\"Build a simple word-word co-occurrence matrix\"\"\"\n",
    "    \n",
    "    corpus = \"\"\"\n",
    "    I love this movie. This movie is great.\n",
    "    I hate that film. That film is terrible.\n",
    "    Great movie, I love it.\n",
    "    Terrible film, I hate it.\n",
    "    \"\"\" * 3  # Repeat for more data\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CO-OCCURRENCE MATRIX\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Tokenize\n",
    "    all_tokens = word_tokenize(corpus.lower())\n",
    "    \n",
    "    # Build vocabulary (top words)\n",
    "    word_counts = Counter(all_tokens)\n",
    "    vocab = [word for word, count in word_counts.most_common(15) \n",
    "             if word.isalpha()]  # Skip punctuation\n",
    "    \n",
    "    print(f\"\\nVocabulary: {vocab}\\n\")\n",
    "    \n",
    "    # Build co-occurrence matrix\n",
    "    window_size = 2\n",
    "    cooccur = defaultdict(Counter)\n",
    "    \n",
    "    for i, word in enumerate(all_tokens):\n",
    "        if word in vocab:\n",
    "            # Look at window around word\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(all_tokens), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j and all_tokens[j] in vocab:\n",
    "                    cooccur[word][all_tokens[j]] += 1\n",
    "    \n",
    "    # Display as matrix\n",
    "    import pandas as pd\n",
    "    \n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    for i, word1 in enumerate(vocab):\n",
    "        for j, word2 in enumerate(vocab):\n",
    "            matrix[i, j] = cooccur[word1][word2]\n",
    "    \n",
    "    df = pd.DataFrame(matrix, index=vocab, columns=vocab)\n",
    "    print(\"Co-occurrence counts (window=2):\")\n",
    "    print(df)\n",
    "    \n",
    "    # Show vectors for specific words\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WORD VECTORS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for word in ['love', 'hate', 'movie', 'film']:\n",
    "        if word in vocab:\n",
    "            vector = df.loc[word]\n",
    "            print(f\"\\nVector for '{word}':\")\n",
    "            print(vector[vector > 0].to_dict())\n",
    "    \n",
    "    # Compute similarity\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COSINE SIMILARITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
    "        dot = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0\n",
    "        return dot / (norm1 * norm2)\n",
    "    \n",
    "    pairs = [\n",
    "        ('love', 'hate'),\n",
    "        ('movie', 'film'),\n",
    "        ('great', 'terrible'),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSimilarity scores:\")\n",
    "    for word1, word2 in pairs:\n",
    "        if word1 in vocab and word2 in vocab:\n",
    "            vec1 = df.loc[word1].values\n",
    "            vec2 = df.loc[word2].values\n",
    "            sim = cosine_similarity(vec1, vec2)\n",
    "            print(f\"  {word1:10} ‚Üî {word2:10}: {sim:.3f}\")\n",
    "        \n",
    "\n",
    "demo_cooccurrence_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06db2f",
   "metadata": {},
   "source": [
    "\n",
    "### OBSERVATIONS\n",
    "\n",
    "- 'movie' and 'film' have high similarity (synonyms!)\n",
    "- Each word is now a vector, not just a one-hot encoding\n",
    "- Still SPARSE (mostly zeros)\n",
    "- Still high dimensional (vocabulary size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb15112",
   "metadata": {},
   "source": [
    "# Dense static embeddings\n",
    "\n",
    "Static embeddings assign exactly one fixed vector to each word, regardless of where or how it is used.\n",
    "\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "- Predictive model\n",
    "- Learns embeddings by predicting surrounding words\n",
    "- Local context focused\n",
    "\n",
    "**skip-gram**\n",
    "predicting context from a word (Skip-gram)\n",
    "\n",
    "\n",
    "**CBOW**\n",
    "predictng a word from context (CBOG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41398921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "documents = [\n",
    "    [\"i\", \"love\", \"this\", \"movie\"],\n",
    "    [\"this\", \"movie\", \"is\", \"great\"],\n",
    "    [\"i\", \"hate\", \"that\", \"film\"],\n",
    "    [\"that\", \"film\", \"is\", \"terrible\"],\n",
    "    [\"great\", \"movie\", \"i\", \"love\", \"it\"],\n",
    "    [\"terrible\", \"film\", \"i\", \"hate\", \"it\"],\n",
    "\n",
    "    # Diverse movie contexts\n",
    "    [\"movie\", \"director\", \"actor\", \"screenplay\"],\n",
    "    [\"film\", \"cinematography\", \"editing\", \"soundtrack\"],\n",
    "    [\"watch\", \"movie\", \"theater\"],\n",
    "    [\"movie\", \"boring\", \"slow\"],\n",
    "    [\"movie\", \"exciting\", \"thrilling\"],\n",
    "    [\"film\", \"award\", \"festival\"],\n",
    "    [\"movie\", \"story\", \"plot\"],\n",
    "    [\"film\", \"critic\", \"review\"],\n",
    "\n",
    "    # Rare words\n",
    "    [\"cinematography\"],\n",
    "    [\"blockbuster\"],\n",
    "    [\"arthouse\", \"film\"],\n",
    "    [\"independent\", \"movie\"]\n",
    "]\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=documents, \n",
    "    vector_size=50, \n",
    "    window=2, \n",
    "    min_count=1, \n",
    "    sg=1,  # Skip-gram model\n",
    "    epochs=100,\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "vector = model.wv['movie']\n",
    "print(vector.shape)\n",
    "print(\"Vector for 'movie':\", vector)\n",
    "print(\"\\nSimilar words to 'movie':\", model.wv.most_similar('movie', topn=5))\n",
    "\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=documents, \n",
    "    vector_size=50, \n",
    "    window=2, \n",
    "    min_count=1, \n",
    "    sg=0,   # CBOW model\n",
    "    epochs=100,\n",
    "    seed=99\n",
    ")\n",
    "\n",
    "print(\"\\nCBOW vector shape:\", cbow_model.wv['movie'].shape)\n",
    "print(\"CBOW similar words to 'movie':\")\n",
    "print(cbow_model.wv.most_similar('movie', topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sg_vec = model.wv['movie']\n",
    "cbow_vec = cbow_model.wv['movie']\n",
    "\n",
    "# Cosine similarity between Skip-gram and CBOW vectors\n",
    "cos_sim = np.dot(sg_vec, cbow_vec) / (\n",
    "    np.linalg.norm(sg_vec) * np.linalg.norm(cbow_vec)\n",
    ")\n",
    "\n",
    "print(\"\\nCosine similarity between Skip-gram and CBOW vectors for 'movie':\")\n",
    "print(f\"{cos_sim:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65b29b",
   "metadata": {},
   "source": [
    "**GloVe**\n",
    "\n",
    "- Count-based + matrix factorization\n",
    "- Uses global co-occurrence statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4ef5f",
   "metadata": {},
   "source": [
    "Unlike co-occurrence vectors, dense embeddings generalize semantic similarity even when exact context overlap is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "embeddings = api.load(\"glove-twitter-25\")\n",
    "\n",
    "print(len(embeddings))            # vocabulary size\n",
    "print(embeddings.vector_size)     # embedding dimension\n",
    "print(embeddings[\"computer\"][5]) # dense vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af695dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.similarity(\"movie\", \"film\"))\n",
    "print(embeddings.similarity(\"movie\", \"pizza\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(\n",
    "    positive=[\"king\", \"woman\"],\n",
    "    negative=[\"man\"],\n",
    "    topn=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2d464",
   "metadata": {},
   "source": [
    "lets consider word bank in two sentences\n",
    "\n",
    "1. I sat by the river bank\n",
    "2. I deposited money by the bank\n",
    "\n",
    "- static embeddings consider bank as same vector\n",
    "- Polysemy leads to contect loss\n",
    "- sentence meaning is not same as word meaning and static encodings operate at the word level\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a8251",
   "metadata": {},
   "source": [
    "#### Contextual Embeddings\n",
    "\n",
    "- Unlike static embeddings (Word2Vec, GloVe), contextual embeddings change depending on the sentence.\n",
    "\n",
    "Example:\n",
    "\n",
    "- ‚ÄúI went to the bank to deposit money‚Äù ‚Üí bank vector reflects financial meaning.\n",
    "- ‚ÄúThe river overflowed the bank‚Äù ‚Üí bank vector reflects riverbank meaning.\n",
    "- Captures polysemy (words with multiple meanings) naturally.\n",
    "\n",
    "\n",
    "**Dimensionality:**\n",
    "\n",
    "- Determined by the model architecture.\n",
    "- Example: all-MiniLM-L6-v2 outputs 384-dimensional vectors for each sentence.\n",
    "- It‚Äôs fixed by the final hidden layer size of the transformer.\n",
    "\n",
    "\n",
    "**reference:**\n",
    "\n",
    "- [Embedding Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "- [sentence transformers](https://sbert.net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install sentence-transformers tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I love playing football.\",\n",
    "    \"Soccer is my favorite sport.\",\n",
    "    \"I enjoy reading books.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Show dimensionality\n",
    "print(\"Embedding shape:\", embeddings[0].shape)  # e.g., (384,)\n",
    "\n",
    "# Compute cosine similarity between sentences\n",
    "similarity_0_1 = util.cos_sim(embeddings[0], embeddings[1])\n",
    "similarity_0_2 = util.cos_sim(embeddings[0], embeddings[2])\n",
    "\n",
    "print(f\"Similarity between sentence 0 and 1: {similarity_0_1.item():.3f}\")\n",
    "print(f\"Similarity between sentence 0 and 2: {similarity_0_2.item():.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
